import calendar
import datetime
from matplotlib import pyplot as plt, dates as mdates
import numpy as np
import os
import pytz
from pytz import timezone
import re
import sys
import threading
import time
import urllib

from linkedin.utils.java import JavaEnvironment, jar_dir_classpath
import linkedin.neelix.java
from linkedin.fetchlog.utils import LogFetcher

##########################
# GLOBAL FUNCTIONS
#########################

def is_valid_url(url):
  """
  Check if a given string is in the correct URL format or not

  :param str url:
  :return: True or False
  """
  regex = re.compile(
      r'^(?:http|ftp)s?://' # http:// or https://
      r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+(?:[A-Z]{2,6}\.?|[A-Z0-9-]{2,}\.?)|' #domain...
      r'localhost|' #localhost...
      r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})' # ...or ip
      r'(?::\d+)?' # optional port
      r'(?:/?|[/?]\S+)$', re.IGNORECASE)
  if regex.match(url):
    print "URL given as config"
    return True
  else:
    return False

def download_file(url):
  """
  Download a file pointed to by url to a temp file on local disk

  :param str url:
  :return: local_file
  """
  try:
    (local_file,headers) = urllib.urlretrieve(url)
  except:
    sys.exit("ERROR: Problem downloading config file. Please check the URL (" + url + "). Exiting...")
  return local_file

def reconcile_timezones(begin_ts, ts_timezone, graph_timezone):
  if not graph_timezone:
    return begin_ts
  # Converting timestamp strings to be in timezone: graph_timezone
  # We only support UTC and PDT
  if graph_timezone != ts_timezone:
    utc = pytz.utc
    pst = timezone('US/Pacific')
    if graph_timezone == "UTC":
      # Assume timezone is PDT
      try:
        dt = pst.localize(datetime.datetime.strptime(begin_ts,"%Y-%m-%d %H:%M:%S"))
      except ValueError:
        dt = pst.localize(datetime.datetime.strptime(begin_ts,"%Y-%m-%d %H:%M:%S.%f"))
      begin_ts = dt.astimezone(utc).strftime("%Y-%m-%d %H:%M:%S.%f")
    else:
      # Assume timezone is UTC since graph_timezone is PDT
      try:
        dt = utc.localize(datetime.datetime.strptime(begin_ts,"%Y-%m-%d %H:%M:%S"))
      except ValueError:
        dt = utc.localize(datetime.datetime.strptime(begin_ts,"%Y-%m-%d %H:%M:%S.%f"))
      begin_ts = dt.astimezone(pst).strftime("%Y-%m-%d %H:%M:%S.%f")
  return begin_ts

def convert_to_unixts(ts_string):
  try:
    dt_obj = datetime.datetime.strptime(ts_string, "%Y-%m-%d %H:%M:%S.%f")
  except ValueError:
    dt_obj = datetime.datetime.strptime(ts_string, "%Y-%m-%d %H:%M:%S")
  return float(calendar.timegm(dt_obj.utctimetuple())*1000.0 + dt_obj.microsecond/1000.0)

def is_number(string):
  try:
    float(string)
    return True
  except ValueError:
    return False

def get_all_sar_objects(metrics, indir, access, output_directory, label, ts_start, ts_end, options):
  metrics = []
  sar_types = ('device', 'cpuusage', 'cpuhz', 'memory', 'memutil', 'paging')
  for sar_metric_type in sar_types:
    #infile = indir + '/' + 'sar.' + sar_metric_type + '.out'
    infile = os.path.join(indir, 'sar.' + sar_metric_type + '.out')
    if os.path.exists(infile):
      obj_type = 'SAR-' + sar_metric_type
      metric = SARMetric(obj_type, infile, access, output_directory, label, ts_start, ts_end, options)
      metrics.append(metric)
  return metrics

def sanitize_string(string):
  string = string.replace('/', '-per-')
  if string.startswith('%'):
    string = string.replace('%', 'percent-')
  else:
    string = string.replace('%', '-percent-')
  return string

def get_default_csv(output_directory, val):
  val = sanitize_string(val)
  return os.path.join(output_directory, val + '.csv')

def convert_to_24hr_format(ts):
  words = ts.split()
  if len(words) == 1:
    return ts
  if words[1] == 'PM':
    tmp = words[0].split(':')
    hour = int(tmp[0]) + 12
    tmp[0] = str(hour)
    ts = ":".join(tmp)
  elif words[1] == 'AM':
    tmp = words[0].split(':')
    if tmp[0] == '12':
      tmp[0] = '00'
    ts = ":".join(tmp)
  return ts

def get_merged_csvname(output_directory, vals):
  return os.path.join(output_directory, '-'.join(vals) + '.csv')

def get_merged_charttitle(vals):
  return " vs ".join(vals)

def get_merged_plot_link_name(vals):
  return '-'.join(vals)

def get_merged_pngname(vals):
  return '-'.join(vals) + '.png'

def convert_to_mdate(s):
  try:
    d = mdates.strpdate2num('%Y-%m-%d %H:%M:%S.%f')(s)
  except:
    d = mdates.strpdate2num('%Y-%m-%d %H:%M:%S')(s)
  return d

#TODO(rmaheshw): use kwargs in all graph_csv methods
def graph_csv_jfreechart(output_directory, csv_file, plot_title, output_filename, y_label=None, precision=None):
  """ Single metric graphing function """
  if not os.path.getsize(csv_file):
    return False
  y_label = y_label or plot_title

  jar_dir = linkedin.neelix.java.generate_jar_dir()
  java_repository = JavaEnvironment(jar_dir_classpath(jar_dir))
  if precision and precision == 'ms':
    java_repository.call('com.linkedin.util.PlotGC',
      "-charts", plot_title, "-cols", y_label,"-in", csv_file, "-out", output_directory,
      "-plotonsamegraph", "true", "-granularity", "ms", "-pngnames", output_filename)
  else:
    java_repository.call('com.linkedin.util.PlotGC',
      "-charts", plot_title, "-cols", y_label,"-in", csv_file, "-out", output_directory,
      "-plotonsamegraph", "true", "-pngnames", output_filename)
  return True

def graph_csv_matplotlib(output_directory, csv_file, plot_title, output_filename, y_label=None, precision=None, graph_height="600", graph_width="1500", graph_type="line", graph_color="black"):
  """ Single metric graphing function using matplotlib"""
  if not os.path.getsize(csv_file):
    return False
  y_label = y_label or plot_title
  days, impressions = np.loadtxt(csv_file, unpack=True, delimiter=",", converters={ 0: convert_to_mdate})
  fig = plt.figure()
  fig.set_size_inches(float(graph_width) / 80, float(graph_height) / 80)
  if graph_type == "line":
    line_style = "-"
    marker = None
  else:
    marker = "."
    line_style = None

  plt.plot_date(x=days, y=impressions, linestyle=line_style, marker=marker, color=graph_color)
  plt.title(plot_title)
  plt.ylabel(y_label)
  plt.grid(True)
  # Get current axis and its xtick labels
  labels = plt.gca().get_xticklabels()
  for label in labels:
    label.set_rotation(30)
  plot_file_name = os.path.join(output_directory, output_filename + ".png")
  fig.savefig(plot_file_name)
  return True

def graph_csv_dygraphs(output_directory, csv_file, plot_title, output_filename, y_label=None, precision=None, graph_height="600", graph_width="1500"):
  """ Single metric graphing function """
  if not os.path.getsize(csv_file):
    return "" 
  y_label = y_label or plot_title
  thread_id = threading.current_thread().ident;
  div_id = str(thread_id) + str(time.time())
  div_string = "<div id=\"%s\" style=\"width:%spx; height:%spx;\"></div>" % (div_id, graph_width, graph_height)
  script_string = """<script type=\"text/javascript\">
        g2 = new Dygraph(
          document.getElementById(\"""" + div_id + """"),
            \"""" + os.path.basename(csv_file) +  """",
            {
                     axes: {
                        x: {
                            valueFormatter: Dygraph.dateString_,
                            valueParser: function(x) { return Date.parseHttpTimeFormat(x); },
                            ticker: Dygraph.dateTicker
                        }
                     },
                        xlabel: "Time",
                        ylabel: \"""" + y_label + """",
                        title: \"""" + plot_title + """",
                        labels: ["Time",\"""" + y_label + """"]
            }          // options
        );
        </script>"""

  # Also generating PNGs if someone needs them separately
  jar_dir = linkedin.neelix.java.generate_jar_dir()
  java_repository = JavaEnvironment(jar_dir_classpath(jar_dir))
  java_repository.call('com.linkedin.util.PlotGC',
      '-charts', plot_title, '-cols', y_label,'-in', csv_file, '-out', output_directory,
      '-plotonsamegraph', 'true', '-pngnames', output_filename)
  return div_string + script_string

def graph_csv_n(perfrepo, output_directory, csv_file, plot_title, output_filename, columns):
  """ graph a csv file with n columns """
  jar_dir = linkedin.neelix.java.generate_jar_dir()
  java_repository = JavaEnvironment(jar_dir_classpath(jar_dir))

  java_repository.call('com.linkedin.util.PlotGC',
      '-charts', plot_title, '-cols', ','.join(columns),'-in', csv_file, '-out', output_directory,
      '-plotonsamegraph', 'true', '-pngnames', output_filename)

def generate_html_report(output_directory, html_string):
  htmlfilename = os.path.join(output_directory, 'Report.html')
  with open(htmlfilename, 'w') as htmlf:
    header = '<html><head>'
    dygraphs_include = '''<script type='text/javascript'
      src='http://dygraphs.com/dygraph-combined.js'></script>
      </head>
      <body>'''
    htmlf.write(header)
    htmlf.write(dygraphs_include)
    htmlf.write(html_string)
    footer = '</body></html>'
    htmlf.write(footer)

def tscsv_nway_file_merge(outfile, filelist, filler):
  print 'called nway merge with'
  print filelist
  with open(outfile, 'w') as outf:
    filehandlers = [None] * len(filelist)
    currlines = [None] * len(filelist)
    for i in range(len(filelist)):
      try:
        filehandlers[i] = open(filelist[i], 'r')
      except IOError:
        print 'Cannot open:', filelist[i]
        return
      currlines[i] = filehandlers[i].readline().strip()
    while True:
      # Assuming logs won't have future dates
      nowtime = str( datetime.datetime.utcnow() )
      min_ts = nowtime
      for i in range(len(currlines)):
        if currlines[i] == "":
          continue
        ts = currlines[i].split(',')[0]
        if ts < min_ts:
          min_ts = ts
      if min_ts == nowtime:
        break
      outwords = []
      outwords.append(min_ts)
      for i in range(len(currlines)):
        if currlines[i] == "":
          outwords.append(filler)
        else:
          ts = currlines[i].split(',')[0]
          val = currlines[i].split(',')[1]
          if ts == min_ts:
            outwords.append(val)
            currlines[i] = filehandlers[i].readline().strip()
          else:
            outwords.append(filler)
      outf.write( ','.join(outwords) + '\n' )

def nway_plotting(crossplots, perfrepo, metrics, output_directory, filler):
  listlen = len(crossplots)
  if listlen == 0:
    return ''
  html_string = []
  linkstring = []
  linkstring.append("<h1><a name=\"Correlated-Plots\"></a>Correlated Plots</h1>\n")
  linkstring.append("<div><ul>")
  i = 0
  #GC.appstop,all GC.alloc,GC.alloc-rate GC.promo,GC.gen0t,GC.gen0sys
  while i < listlen:
    plot = crossplots[i]
    vals = plot.split(',')
    i += 1
    if not 'all' in vals:
      csvfiles = []
      for val in vals:
        csv_file = get_default_csv(output_directory, val)
        csvfiles.append(csv_file)
      for j in range(len(vals)):
        vals[j] = sanitize_string(vals[j])
      mergedfilename = get_merged_csvname(output_directory, vals)
      plot_title = get_merged_charttitle(vals)
      pngname = get_merged_plot_link_name(vals)
      merged_plotfile = get_merged_pngname(vals)

      tscsv_nway_file_merge(mergedfilename, csvfiles, filler)
      graph_csv_n(perfrepo, output_directory, mergedfilename, plot_title, pngname, vals)

      img_tag = "<h3><a name=\"{0}\"></a>{1}</h3><img src={2} />".format(pngname, plot_title, merged_plotfile)
      linktag = "<li><a href=\"#{0}\">{1}</a></li>".format(pngname, plot_title)
      html_string.append(img_tag)
      linkstring.append(linktag)
    else:
      vals.remove('all')
      for metric in metrics:
        for csv in metric.csvfiles:
          csv_filename = csv.split('/')[-1]
          metric_name = '.'.join(csv_filename.split('.')[0:-1])
          if metric_name in vals:
            continue
          new_val = []
          new_val.extend(vals)
          new_val.append(metric_name)
          new_val_str = ','.join(new_val)
          crossplots.append(new_val_str)
          listlen += 1
  linkstring.append("</ul></div>")
  linkstring.extend(html_string)
  return '\n'.join(linkstring)

##########################
# CLASS DEFINITIONS
#########################

class Metric(object):
  beginning_ts = None
  beginning_date = None
  ignore = False
  timezone = "PDT"
  # Parsed csv files
  csvfiles = []
  metric_description = []

  def __init__ (self, metric_type, infile, inaccess, output_directory, label, ts_start, ts_end):
    self.metric_type = metric_type
    self.infile = infile
    self.inaccess = inaccess
    self.outdir = output_directory
    self.label = label
    self.ts_start = ts_start
    self.ts_end = ts_end
    self.calc_metrics = None
    self.precision = None

  def ts_out_of_range(self, timestamp):
    if self.ts_start and timestamp < self.ts_start:
      return True
    elif self.ts_end and timestamp > self.ts_end:
      return True
    return False

  def collect_local(self):
    return os.path.exists(self.infile)

  def fetch_log(self):
    fetcher = LogFetcher(self.outdir, password=self.passphrase, key=self.ssh_key_location)
    remotedir, filename = os.path.split(self.infile)
    re_filename = re.compile(filename + '$')
    print 'Fetching log:', self.hostname, remotedir, re_filename.pattern, self.outdir, filename
    dir_list = fetcher.fetch_logs_by_host([self.hostname], remotedir, re_filename)
    self.infile = os.path.join(dir_list[0]['path'], filename)
    if dir_list[0]['is_fetched']:
      return self.collect_local()
    else:
      return None

  def collect(self):
    if self.inaccess == 'local':
      return self.collect_local()
    elif self.inaccess == 'ssh':
      #Also change start and end times to UTC if fetching from PROD
      if '.prod.' in self.hostname:
        self.timezone = "UTC"
      return self.fetch_log()
    else:
      print "WARNING: access is set to other than local or ssh for metric", self.label
      return False

  def get_csv(self, column):
    col = sanitize_string(column)
    csv = os.path.join(self.outdir, self.metric_type + '.' + col + '.csv')
    return csv

  def parse(self):
    print "Working on", self.infile
    with open(self.infile, 'r') as infile:
      data = {}
      for line in infile:
        if self.sep is None:
          words = line.strip().split()
        else:
          words = line.strip().split(self.sep)
        if len(words) == 0:
          continue
        if len(words) < len(self.columns):
          print "ERROR: Number of columns given in config is more than number of columns present in file {0}\n".format(self.infile)
          sys.exit(1)
        ts = linkedin.neelix.metric.reconcile_timezones(words[0], self.timezone, self.graph_timezone)
        for i in range(len(self.columns)):
          out_csv = self.get_csv(self.columns[i])
          if out_csv in data:
            data[out_csv].append( ts + ',' + words[i+1] )
          else:
            data[out_csv] = []
            data[out_csv].append( ts + ',' + words[i+1] )
    # Post processing, putting data in csv files
    for csv in data.keys():
      self.csvfiles.append(csv)
      with open(csv, 'w') as fh:
        fh.write('\n'.join(data[csv]))
    return True

  def calc(self):
    if not self.calc_metrics:
      return
    calculation_array = self.calc_metrics.split()
    for calculation in calculation_array:
      words = calculation.split('=')
      newmetric = words[0]
      expr = words[1]
      p = re.compile('(\w+)\((.+)\)')
      calc_type = p.match(expr).group(1)
      old_metric = p.match(expr).group(2)
      print newmetric, expr, old_metric, calc_type
      if not calc_type in ('rate', 'diff'):
        print "ERROR: Invalid calc_metric type {0} defined in config".format(calc_type)
        continue
      old_metric_csv = self.get_csv(old_metric)
      new_metric_csv = self.get_csv(newmetric)
      self.csvfiles.append(new_metric_csv)
      oldval = None
      with open(old_metric_csv, 'r') as FH:
        with open(new_metric_csv, 'w') as NEW_FH:
          for line in FH:
            w = line.split(',')
            ts = w[0]
            val = w[1]
            if not oldval:
              oldts = ts
              oldval = val
              continue
            if calc_type == 'rate':
              new_metric_val = (float(val) - float(oldval))/(convert_to_unixts(ts) - convert_to_unixts(oldts))
            elif calc_type == 'diff':
              new_metric_val = (float(val) - float(oldval))
            oldts = ts
            oldval = val
            NEW_FH.write(ts)
            NEW_FH.write(',')
            NEW_FH.write(str(new_metric_val))
            NEW_FH.write('\n')

  def graph(self, perfrepo, graphing_library = 'matplotlib'):
    html_string = []
    html_string.append('<h1>Metric: {0}</h1>\n'.format(self.metric_type))
    graphed = False
    for out_csv in self.csvfiles:
      csv_filename = os.path.basename(out_csv)
      # The last element is .csv, don't need that in the name of the chart
      graph_title = '.'.join(csv_filename.split('.')[0:-1])
      column = '.'.join(graph_title.split('.')[1:])
      if graphing_library in ('dygraphs', 'javascript', 'js'):
        html_string.append(graph_csv_dygraphs(self.outdir, out_csv, graph_title, graph_title))
      else:
        if graphing_library in ('matplotlib'):
          graphed = graph_csv_matplotlib(self.outdir, out_csv, graph_title, graph_title)
        else:
          graphed = graph_csv_jfreechart(self.outdir, out_csv, graph_title, graph_title)
        if graphed:
          if column in self.metric_description:
            img_tag = "<h3>{title}</h3><p><b>Description</b>: {description}</p><img src={image_name}.png />\n".format(title=graph_title, description=self.metric_description[column], image_name=graph_title)
          else:
            img_tag = "<h3>{title}</h3><img src={image_name} />\n".format(title=graph_title, image_name=graph_title + '.png')
        else:
          img_tag = "<h3>No data for this metric</h3>"
        html_string.append(img_tag)
    return '\n'.join(html_string)
