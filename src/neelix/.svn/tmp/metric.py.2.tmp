import calendar
import datetime
import os
import pytz
from pytz import timezone
import re
import sys

from linkedin.utils.java import JavaEnvironment, jar_dir_classpath
import linkedin.neelix.java
from linkedin.fetchlog.utils import LogFetcher

##########################
# GLOBAL FUNCTIONS
#########################

def reconcile_timezones(begin_ts, ts_timezone, graph_timezone):
  # Converting timestamp strings to be in timezone: graph_timezone
  # We only support UTC and PDT
  if graph_timezone != ts_timezone:
    utc = pytz.utc
    pst = timezone('US/Pacific')
    if graph_timezone == "UTC":
      # Assume timezone is PDT
      try:
        dt = pst.localize(datetime.datetime.strptime(begin_ts,"%Y-%m-%d %H:%M:%S"))
      except ValueError:
        dt = pst.localize(datetime.datetime.strptime(begin_ts,"%Y-%m-%d %H:%M:%S.%f"))
      begin_ts = dt.astimezone(utc).strftime("%Y-%m-%d %H:%M:%S.%f")
    else:
      # Assume timezone is UTC since graph_timezone is PDT
      try:
        dt = utc.localize(datetime.datetime.strptime(begin_ts,"%Y-%m-%d %H:%M:%S"))
      except ValueError:
        dt = utc.localize(datetime.datetime.strptime(begin_ts,"%Y-%m-%d %H:%M:%S.%f"))
      begin_ts = dt.astimezone(pst).strftime("%Y-%m-%d %H:%M:%S.%f")
  return begin_ts

def convert_to_unixts(ts_string):
  try:
    dt_obj = datetime.datetime.strptime(ts_string, "%Y-%m-%d %H:%M:%S.%f")
  except ValueError:
    dt_obj = datetime.datetime.strptime(ts_string, "%Y-%m-%d %H:%M:%S")
  return float(calendar.timegm(dt_obj.utctimetuple())*1000.0 + dt_obj.microsecond/1000.0)

def is_number(string):
  try:
    float(string)
    return True
  except ValueError:
    return False

def get_all_sar_objects(metrics, indir, access, outdir, label, ts_start, ts_end, options):
  metrics = []
  sar_types = ('device', 'cpuusage', 'cpuhz', 'memory', 'memutil', 'paging')
  for sar_metric_type in sar_types:
    #infile = indir + '/' + 'sar.' + sar_metric_type + '.out'
    infile = os.path.join(indir, 'sar.' + sar_metric_type + '.out')
    if os.path.exists(infile):
      obj_type = 'SAR-' + sar_metric_type
      metric = SARMetric(obj_type, infile, access, outdir, label, ts_start, ts_end, options)
      metrics.append(metric)
  return metrics

def sanitize_string(string):
  string = string.replace('/', '-per-')
  if string.startswith('%'):
    string = string.replace('%', 'percent-')
  else:
    string = string.replace('%', '-percent-')
  return string

def get_default_csv(outdir, val):
  val = sanitize_string(val)
  return os.path.join(outdir, val + '.csv')

def convert_to_24hr_format(ts):
  words = ts.split()
  if len(words) == 1:
    return ts
  if words[1] == 'PM':
    tmp = words[0].split(':')
    hour = int(tmp[0]) + 12
    tmp[0] = str(hour)
    ts = ":".join(tmp)
  elif words[1] == 'AM':
    tmp = words[0].split(':')
    if tmp[0] == '12':
      tmp[0] = '00'
    ts = ":".join(tmp)
  return ts

def get_merged_csvname(outdir, vals):
  return os.path.join(outdir, '-'.join(vals) + '.csv')

def get_merged_charttitle(vals):
  return " vs ".join(vals)

def get_merged_plot_link_name(vals):
  return '-'.join(vals)

def get_merged_pngname(vals):
  return '-'.join(vals) + '.png'

def graph_csv(perfrepo, outdir, csvfile, plot_title, outfilename, ylabel=None, precision=None):
  ''' Single metric graphing function '''
  if os.path.getsize(csvfile) == 0:
    return False
  jar_dir = linkedin.neelix.java.generate_jar_dir()
  java_repository = JavaEnvironment(jar_dir_classpath(jar_dir))

  if not ylabel:
    ylabel = plot_title
  if precision and precision == 'ms':
    java_repository.call('com.linkedin.util.PlotGC',
      "-charts", plot_title, "-cols", ylabel,"-in", csvfile, "-out", outdir,
      "-plotonsamegraph", "true", "-granularity", "ms", "-pngnames", outfilename)
  else:
    java_repository.call('com.linkedin.util.PlotGC',
      "-charts", plot_title, "-cols", ylabel,"-in", csvfile, "-out", outdir,
      "-plotonsamegraph", "true", "-pngnames", outfilename)
  return True

def graph_csv_n(perfrepo, outdir, csvfile, plot_title, outfilename, columns):
  ''' graph a csv file with n columns '''
  jar_dir = linkedin.neelix.java.generate_jar_dir()
  java_repository = JavaEnvironment(jar_dir_classpath(jar_dir))

  java_repository.call('com.linkedin.util.PlotGC',
      "-charts", plot_title, "-cols", ','.join(columns),"-in", csvfile, "-out", outdir,
      "-plotonsamegraph", "true", "-pngnames", outfilename)

def generate_html_report(outdir, htmlstring):
  htmlfilename = os.path.join(outdir, 'Report.html')
  with open(htmlfilename, 'w') as htmlf:
    header = "<html><head /><body>"
    htmlf.write(header)
    htmlf.write(htmlstring)
    footer = "</body></html>"
    htmlf.write(footer)

def tscsv_nway_file_merge(outfile, filelist, filler):
  print 'called nway merge with'
  print filelist
  with open(outfile, 'w') as outf:
    filehandlers = [None] * len(filelist)
    currlines = [None] * len(filelist)
    for i in range(len(filelist)):
      try:
        filehandlers[i] = open(filelist[i], 'r')
      except IOError:
        print "Cannot open:", filelist[i]
        return
      currlines[i] = filehandlers[i].readline().strip()
    while True:
      # Assuming logs won't have future dates
      nowtime = str( datetime.datetime.utcnow() )
      min_ts = nowtime
      for i in range(len(currlines)):
        if currlines[i] == "":
          continue
        ts = currlines[i].split(',')[0]
        if ts < min_ts:
          min_ts = ts
      if min_ts == nowtime:
        break
      outwords = []
      outwords.append(min_ts)
      for i in range(len(currlines)):
        if currlines[i] == "":
          outwords.append(filler)
        else:
          ts = currlines[i].split(',')[0]
          val = currlines[i].split(',')[1]
          if ts == min_ts:
            outwords.append(val)
            currlines[i] = filehandlers[i].readline().strip()
          else:
            outwords.append(filler)
      outf.write( ','.join(outwords) + '\n' )

def nway_plotting(crossplots, perfrepo, metrics, outdir, filler):
  listlen = len(crossplots)
  if listlen == 0:
    return ""
  htmlstring = []
  linkstring = []
  linkstring.append("<h1><a name=\"Correlated-Plots\"></a>Correlated Plots</h1>\n")
  linkstring.append("<div><ul>")
  i = 0
  #GC.appstop,all GC.alloc,GC.alloc-rate GC.promo,GC.gen0t,GC.gen0sys
  while i < listlen:
    plot = crossplots[i]
    vals = plot.split(',')
    i += 1
    if not 'all' in vals:
      csvfiles = []
      for val in vals:
        csvfile = get_default_csv(outdir, val)
        csvfiles.append(csvfile)
      for j in range(len(vals)):
        vals[j] = sanitize_string(vals[j])
      mergedfilename = get_merged_csvname(outdir, vals)
      plot_title = get_merged_charttitle(vals)
      pngname = get_merged_plot_link_name(vals)
      merged_plotfile = get_merged_pngname(vals)

      tscsv_nway_file_merge(mergedfilename, csvfiles, filler)
      graph_csv_n(perfrepo, outdir, mergedfilename, plot_title, pngname, vals)

      #imgtag = "<h3><a name=\"%s\"></a>%s</h3><img src=%s />" % (pngname, plot_title, merged_plotfile)
      #linktag = "<li><a href=\"#%s\">%s</a></li>" % (pngname, plot_title)
      imgtag = "<h3><a name=\"{0}\"></a>{1}</h3><img src={2} />".format(pngname, plot_title, merged_plotfile)
      linktag = "<li><a href=\"#{0}\">{1}</a></li>".format(pngname, plot_title)
      htmlstring.append(imgtag)
      linkstring.append(linktag)
    else:
      vals.remove('all')
      for metric in metrics:
        for csv in metric.csvfiles:
          csvfilename = csv.split('/')[-1]
          x = '.'.join(csvfilename.split('.')[0:-1])
          if x in vals:
            continue
          new_val = []
          new_val.extend(vals)
          new_val.append(x)
          new_val_str = ','.join(new_val)
          crossplots.append(new_val_str)
          #print "crossplots"
          #print crossplots
          listlen += 1
  linkstring.append("</ul></div>")
  linkstring.extend(htmlstring)
  return '\n'.join(linkstring)

##########################
# CLASS DEFINITIONS
#########################

class Metric(object):
  beginning_ts = None
  beginning_date = None
  ignore = False
  timezone = "PDT"
  # Parsed csv files
  csvfiles = []

  def __init__ (self, metric_type, infile, inaccess, outdir, label, ts_start, ts_end):
    self.metric_type = metric_type
    self.infile = infile
    self.inaccess = inaccess
    self.outdir = outdir
    self.label = label
    self.ts_start = ts_start
    self.ts_end = ts_end
    self.calc_metrics = None
    self.precision = None

  def ts_out_of_range(self, timestamp):
    if self.ts_start and timestamp < self.ts_start:
      return True
    elif self.ts_end and timestamp > self.ts_end:
      return True
    return False

  def collect_local(self):
    return os.path.exists(self.infile)

  def fetch_log(self):
    fetcher = LogFetcher(self.outdir, password=self.passphrase, key_filename=self.ssh_key_location)
    remotedir, filename = os.path.split(self.infile)
    re_filename = re.compile(filename + '$')
    print 'Fetching log:', self.hostname, remotedir, re_filename.pattern, self.outdir, filename
    dir_list = fetcher.fetch_logs_by_host([self.hostname], remotedir, re_filename)
    self.infile = os.path.join(dir_list[0], filename)
    return self.collect_local()

  def collect(self):
    if self.inaccess == 'local':
      return self.collect_local()
    elif self.inaccess == 'ssh':
      #Also change start and end times to UTC if fetching from PROD
      if '.prod.' in self.hostname:
        self.timezone = "UTC"
      return self.fetch_log()
    else:
      print "WARNING: access is set to other than local or ssh for metric", self.label
      return False

  def get_csv(self, column):
    col = sanitize_string(column)
    csv = os.path.join(self.outdir, self.metric_type + '.' + col + '.csv')
    return csv

  def parse(self):
    print "Working on", self.infile
    with open(self.infile, 'r') as infile:
      data = {}
      for line in infile:
        if self.sep is None:
          words = line.strip().split()
        else:
          words = line.strip().split(self.sep)
        if len(words) == 0:
          continue
        if len(words) < len(self.columns):
          print "ERROR: Number of columns given in config is more than number of columns present in file {0}\n".format(self.infile)
          sys.exit(1)
        ts = linkedin.neelix.metric.reconcile_timezones(words[0], self.timezone, self.graph_timezone)
        for i in range(len(self.columns)):
          outcsv = self.get_csv(self.columns[i])
          if outcsv in data:
            data[outcsv].append( ts + ',' + words[i+1] )
          else:
            data[outcsv] = []
            data[outcsv].append( ts + ',' + words[i+1] )
    # Post processing, putting data in csv files
    for csv in data.keys():
      self.csvfiles.append(csv)
      with open(csv, 'w') as fh:
        fh.write('\n'.join(data[csv]))
    return True

  def calc(self):
    if not self.calc_metrics:
      return
    calculation_array = self.calc_metrics.split()
    for calculation in calculation_array:
      words = calculation.split('=')
      newmetric = words[0]
      expr = words[1]
      p = re.compile('(\w+)\((.+)\)')
      calc_type = p.match(expr).group(1)
      old_metric = p.match(expr).group(2)
      print newmetric, expr, old_metric, calc_type
      if not calc_type in ('rate', 'diff'):
        print "ERROR: Invalid calc_metric type {0} defined in config".format(calc_type)
        continue
      old_metric_csv = self.get_csv(old_metric)
      new_metric_csv = self.get_csv(newmetric)
      self.csvfiles.append(new_metric_csv)
      oldval = None
      with open(old_metric_csv, 'r') as FH:
        with open(new_metric_csv, 'w') as NEW_FH:
          for line in FH:
            w = line.split(',')
            ts = w[0]
            val = w[1]
            if not oldval:
              oldts = ts
              oldval = val
              continue
            if calc_type == 'rate':
              new_metric_val = (float(val) - float(oldval))/(convert_to_unixts(ts) - convert_to_unixts(oldts))
            elif calc_type == 'diff':
              new_metric_val = (float(val) - float(oldval))
            oldts = ts
            oldval = val
            NEW_FH.write(ts)
            NEW_FH.write(',')
            NEW_FH.write(str(new_metric_val))
            NEW_FH.write('\n')

  def graph(self, perfrepo):
    htmlstring = []
    htmlstring.append("<h2>Metric: {0}</h2>\n".format(self.metric_type))
    for column in self.columns:
      outcsv = self.get_csv(column)
      pngname = self.metric_type + '.' + column
      pngname = sanitize_string(pngname)
      title = self.titles[column]
      ylabel = self.ylabels[column]
      graph_csv(perfrepo, self.outdir, outcsv, title, pngname, ylabel, self.precision)
      imgtag = "<h3>{0}</h3><img src={1} />\n".format(pngname, pngname + '.png')
      htmlstring.append(imgtag)
    return '\n'.join(htmlstring)
